{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, I implement a Q-learning algorithm to solve the MountainCar-v0 game.\n",
    "\n",
    "I tried two experiments, one is `how learning rate change affect the results`, and the other is `how state number of q-matrix affect the results`.\n",
    "\n",
    "And I provided three diagrams to illustrate the rusults, which are `fail times over training process` (fail meant the car didn't catch the flag), `rewards over traing process` and `step number in each episode over traing process`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result and Analysis\n",
    "\n",
    "the basic environments is learning rate in 0.01, state number is 15, maximum step number is 1500 and training episode is 5000.\n",
    "\n",
    "## with different learning rate\n",
    "\n",
    "### fail times\n",
    "\n",
    "- learning rate = 0.01\n",
    "![](./fig/learning_rate0.01-fail.png)\n",
    "\n",
    "- learning rate = 0.05\n",
    "![](./fig/learning_rate0.05-fail.png)\n",
    "\n",
    "- learning rate = 0.1\n",
    "![](./fig/learning_rate0.1-fail.png)\n",
    "\n",
    "as you can see, the fail times as first reduce as learning rate increase, but with a long training period they are no different.\n",
    "\n",
    "\n",
    "### reward\n",
    "- learning rate = 0.01\n",
    "![](./fig/learning_rate0.01-reward.png)\n",
    "\n",
    "- learning rate = 0.05\n",
    "![](./fig/learning_rate0.05-reward.png)\n",
    "\n",
    "- learning rate = 0.1\n",
    "![](./fig/learning_rate0.1-reward.png)\n",
    "\n",
    "as you can see, the reward reduce as learning rate increase, and seems learning rate = 0.05 would be a magic number of this game.\n",
    "\n",
    "### step number\n",
    "\n",
    "- learning rate = 0.01\n",
    "![](./fig/learning_rate0.01-step.png)\n",
    "- learning rate = 0.05\n",
    "![](./fig/learning_rate0.05-step.png)\n",
    "- learning rate = 0.1\n",
    "![](./fig/learning_rate0.1-step.png)\n",
    "\n",
    "step number of learning rate = 0.01 is obviously higher than other two environment, but learning rate = 0.05 and 0.1 are no such obviously differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with different state number of q-matrix\n",
    "\n",
    "### fail times\n",
    "\n",
    "- state number = 5 * 5\n",
    "![](./fig/state5-fail.png)\n",
    "\n",
    "- learning rate = 10 * 10\n",
    "![](./fig/state10-fail.png)\n",
    "\n",
    "- learning rate = 15 * 15\n",
    "![](./fig/state15-fail.png)\n",
    "\n",
    "- learning rate = 20 * 20\n",
    "![](./fig/state20-fail.png)\n",
    "\n",
    "the state number of 5 * 5 q-matrix was not working, and 20 * 20 can perfore well relatively.\n",
    "\n",
    "\n",
    "### reward\n",
    "- state number = 5 * 5\n",
    "![](./fig/state5-reward.png)\n",
    "\n",
    "- learning rate = 10 * 10\n",
    "![](./fig/state10-reward.png)\n",
    "\n",
    "- learning rate = 15 * 15\n",
    "![](./fig/state15-reward.png)\n",
    "\n",
    "- learning rate = 20 * 20\n",
    "![](./fig/state20-reward.png)\n",
    "\n",
    "though the 5 * 5 q-matrix still not working, but performance of 20 * 20 q-matrix is worse than 15 * 15 or 10 * 10. A intuited reason is we have so much states so the agent just doesn't know how to select well at first, so it may need more training episode to solve.\n",
    "\n",
    "### step number\n",
    "\n",
    "- state number = 5 * 5\n",
    "![](./fig/state5-step.png)\n",
    "\n",
    "- learning rate = 10 * 10\n",
    "![](./fig/state10-step.png)\n",
    "\n",
    "- learning rate = 15 * 15\n",
    "![](./fig/state15-step.png)\n",
    "\n",
    "- learning rate = 20 * 20\n",
    "![](./fig/state20-step.png)\n",
    "\n",
    "step number of learning rate = 0.01 is obviously higher than other two environment, but learning rate = 0.05 and 0.1 are no such obviously differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers about questions\n",
    "After analyzing the experiment, please answer the following questions in your report (30%)\n",
    "\n",
    "1. What kind of RL algorithms did you use? value-based, policy-based, model-based? why? (10%)\n",
    "```\n",
    "ANS:\n",
    "I used Q-learning, a value-based reinforcement learning algorithm, which meant it didn't any policy selection function to play the game, and I choosed Q-learning algorithm just because it was easy to implement.\n",
    "```\n",
    "2. This algorithms is off-policy or on-policy? why? (10%)\n",
    "```\n",
    "ANS:\n",
    "Q-learning is a off-policy reinforcement algorithm, because it use maximum Q-value which meant it was not independent.\n",
    "```\n",
    "3. How does your algorithm solve the correlation problem in the same MDP? (10%)\n",
    "```\n",
    "ANS:\n",
    "Q-matrix is simply equal to MDP. and due to lack of state number of q-matrix compared to NN, the Q-learning have no urgently demand to solving overfitting problem of correlation.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
